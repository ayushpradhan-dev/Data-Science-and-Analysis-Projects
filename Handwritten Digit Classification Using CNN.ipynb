{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayush\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\ayush\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import core libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Import model components\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPool2D, BatchNormalization, \n",
    "                                   Dropout, Dense, Flatten, GaussianNoise)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import (LearningRateScheduler, \n",
    "                                      ModelCheckpoint, EarlyStopping)\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Optimizer with weight decay from TensorFlow Addons\n",
    "from tensorflow_addons.optimizers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit GPU memory available to TensorFlow\n",
    "\n",
    "# Get list of GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=20480)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape data to (num_samples, height, width, channels) and normalize to [0,1]\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "\n",
    "# Convert class labels to one-hot encoded vectors\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data augmentation pipeline\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,       # Random rotations ±15 degrees\n",
    "    zoom_range=0.2,          # Random zoom up to 20%\n",
    "    width_shift_range=0.15,  # Horizontal shift ±15% of width\n",
    "    height_shift_range=0.15, # Vertical shift ±15% of height\n",
    "    shear_range=0.1,         # Shear transformations\n",
    "    validation_split=0.2,    # Hold out 20% for validation\n",
    "    preprocessing_function=lambda x: x + np.random.normal(0, 0.05, x.shape) # Add Gaussian noise\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN architecture\n",
    "net = Sequential([\n",
    "    # Input layer with Gaussian noise for regularization\n",
    "    GaussianNoise(0.1, input_shape=(28,28,1)),\n",
    "    \n",
    "    # First convolutional block\n",
    "    Conv2D(64, (3,3), activation='swish', padding='same', kernel_regularizer=l2(1e-4)),\n",
    "    BatchNormalization(),  # Normalize activations before passing to next layer\n",
    "    Conv2D(64, (3,3), activation='swish', padding='same', kernel_regularizer=l2(1e-4)),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),      # Downsample by factor of 2 in both dimensions\n",
    "    Dropout(0.25),         # Randomly set 25% of activations to zero\n",
    "    \n",
    "    # Second convolutional block with more filters\n",
    "    Conv2D(128, (3,3), activation='swish', padding='same', kernel_regularizer=l2(1e-4)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3,3), activation='swish', padding='same', kernel_regularizer=l2(1e-4)),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),\n",
    "    Dropout(0.35),        # Increased dropout for deeper layers to prevent overfitting\n",
    "    \n",
    "    # Final convolutional block with even more filters\n",
    "    Conv2D(256, (3,3), activation='swish', padding='valid', kernel_regularizer=l2(1e-4)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3,3), activation='swish', padding='valid', kernel_regularizer=l2(1e-4)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),         # High dropout before dense layers to prevent overfitting\n",
    "    \n",
    "    # Classification head with dense layers\n",
    "    Flatten(),            # Convert 3D features to 1D for dense layers\n",
    "    Dense(1024, activation='swish', kernel_regularizer=l2(1e-4)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, activation='swish', kernel_regularizer=l2(1e-4)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(10, activation='softmax')  # Output layer with class probabilities, 10 different classes (digits 0 through 9) with softmax activation (assigns class with highest probability)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedule function for training\n",
    "def lr_schedule(epoch):\n",
    "    lr = 3e-4   # Initial learning rate for AdamW optimizer\n",
    "    if epoch > 30: lr *= 0.1    # Reduce LR by 10x after 30 epochs \n",
    "    if epoch > 45: lr *= 0.1    # Reduce again after 45 epochs \n",
    "    return lr\n",
    "\n",
    "# Configure model for training\n",
    "net.compile(optimizer=AdamW(weight_decay=1e-5),   # Adam with weight decay optimizer \n",
    "              loss='categorical_crossentropy',    # Suitable for multi-class classification \n",
    "              metrics=['accuracy'])               # Monitor accuracy during training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "188/188 [==============================] - 19s 76ms/step - loss: 1.5960 - accuracy: 0.6064 - val_loss: 10.7308 - val_accuracy: 0.1126 - lr: 3.0000e-04\n",
      "Epoch 2/60\n",
      "188/188 [==============================] - 14s 76ms/step - loss: 0.6160 - accuracy: 0.8956 - val_loss: 2.5713 - val_accuracy: 0.3443 - lr: 3.0000e-04\n",
      "Epoch 3/60\n",
      "188/188 [==============================] - 16s 85ms/step - loss: 0.4918 - accuracy: 0.9336 - val_loss: 0.5301 - val_accuracy: 0.9183 - lr: 3.0000e-04\n",
      "Epoch 4/60\n",
      "188/188 [==============================] - 15s 82ms/step - loss: 0.4323 - accuracy: 0.9527 - val_loss: 0.4445 - val_accuracy: 0.9527 - lr: 3.0000e-04\n",
      "Epoch 5/60\n",
      "188/188 [==============================] - 16s 85ms/step - loss: 0.3987 - accuracy: 0.9611 - val_loss: 0.3598 - val_accuracy: 0.9758 - lr: 3.0000e-04\n",
      "Epoch 6/60\n",
      "188/188 [==============================] - 16s 83ms/step - loss: 0.3744 - accuracy: 0.9679 - val_loss: 0.3608 - val_accuracy: 0.9749 - lr: 3.0000e-04\n",
      "Epoch 7/60\n",
      "188/188 [==============================] - 16s 86ms/step - loss: 0.3587 - accuracy: 0.9703 - val_loss: 0.3313 - val_accuracy: 0.9797 - lr: 3.0000e-04\n",
      "Epoch 8/60\n",
      "188/188 [==============================] - 15s 81ms/step - loss: 0.3456 - accuracy: 0.9732 - val_loss: 0.3408 - val_accuracy: 0.9769 - lr: 3.0000e-04\n",
      "Epoch 9/60\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.3267 - accuracy: 0.9768 - val_loss: 0.3461 - val_accuracy: 0.9747 - lr: 3.0000e-04\n",
      "Epoch 10/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.3169 - accuracy: 0.9786 - val_loss: 0.3139 - val_accuracy: 0.9818 - lr: 3.0000e-04\n",
      "Epoch 11/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.3099 - accuracy: 0.9786 - val_loss: 0.3084 - val_accuracy: 0.9805 - lr: 3.0000e-04\n",
      "Epoch 12/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.2961 - accuracy: 0.9806 - val_loss: 0.2896 - val_accuracy: 0.9836 - lr: 3.0000e-04\n",
      "Epoch 13/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.2859 - accuracy: 0.9816 - val_loss: 0.2742 - val_accuracy: 0.9857 - lr: 3.0000e-04\n",
      "Epoch 14/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.2751 - accuracy: 0.9828 - val_loss: 0.2618 - val_accuracy: 0.9883 - lr: 3.0000e-04\n",
      "Epoch 15/60\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.2660 - accuracy: 0.9836 - val_loss: 0.2654 - val_accuracy: 0.9843 - lr: 3.0000e-04\n",
      "Epoch 16/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.2567 - accuracy: 0.9843 - val_loss: 0.2491 - val_accuracy: 0.9868 - lr: 3.0000e-04\n",
      "Epoch 17/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.2508 - accuracy: 0.9835 - val_loss: 0.2428 - val_accuracy: 0.9865 - lr: 3.0000e-04\n",
      "Epoch 18/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.2385 - accuracy: 0.9852 - val_loss: 0.2272 - val_accuracy: 0.9887 - lr: 3.0000e-04\n",
      "Epoch 19/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.2314 - accuracy: 0.9851 - val_loss: 0.2256 - val_accuracy: 0.9871 - lr: 3.0000e-04\n",
      "Epoch 20/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.2255 - accuracy: 0.9849 - val_loss: 0.2157 - val_accuracy: 0.9865 - lr: 3.0000e-04\n",
      "Epoch 21/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.2125 - accuracy: 0.9866 - val_loss: 0.2110 - val_accuracy: 0.9878 - lr: 3.0000e-04\n",
      "Epoch 22/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.2031 - accuracy: 0.9880 - val_loss: 0.1948 - val_accuracy: 0.9903 - lr: 3.0000e-04\n",
      "Epoch 23/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.1955 - accuracy: 0.9872 - val_loss: 0.1900 - val_accuracy: 0.9897 - lr: 3.0000e-04\n",
      "Epoch 24/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.1902 - accuracy: 0.9871 - val_loss: 0.1851 - val_accuracy: 0.9882 - lr: 3.0000e-04\n",
      "Epoch 25/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.1830 - accuracy: 0.9878 - val_loss: 0.1728 - val_accuracy: 0.9898 - lr: 3.0000e-04\n",
      "Epoch 26/60\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.1734 - accuracy: 0.9886 - val_loss: 0.1800 - val_accuracy: 0.9866 - lr: 3.0000e-04\n",
      "Epoch 27/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.1674 - accuracy: 0.9888 - val_loss: 0.1566 - val_accuracy: 0.9921 - lr: 3.0000e-04\n",
      "Epoch 28/60\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.1594 - accuracy: 0.9893 - val_loss: 0.1614 - val_accuracy: 0.9896 - lr: 3.0000e-04\n",
      "Epoch 29/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.1579 - accuracy: 0.9881 - val_loss: 0.1503 - val_accuracy: 0.9904 - lr: 3.0000e-04\n",
      "Epoch 30/60\n",
      "188/188 [==============================] - 14s 74ms/step - loss: 0.1470 - accuracy: 0.9897 - val_loss: 0.1438 - val_accuracy: 0.9902 - lr: 3.0000e-04\n",
      "Epoch 31/60\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.1435 - accuracy: 0.9896 - val_loss: 0.1448 - val_accuracy: 0.9883 - lr: 3.0000e-04\n",
      "Epoch 32/60\n",
      "188/188 [==============================] - 14s 74ms/step - loss: 0.1337 - accuracy: 0.9911 - val_loss: 0.1261 - val_accuracy: 0.9936 - lr: 3.0000e-05\n",
      "Epoch 33/60\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.1284 - accuracy: 0.9923 - val_loss: 0.1279 - val_accuracy: 0.9922 - lr: 3.0000e-05\n",
      "Epoch 34/60\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.1245 - accuracy: 0.9933 - val_loss: 0.1261 - val_accuracy: 0.9928 - lr: 3.0000e-05\n",
      "Epoch 35/60\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.1224 - accuracy: 0.9933 - val_loss: 0.1218 - val_accuracy: 0.9935 - lr: 3.0000e-05\n",
      "Epoch 36/60\n",
      "188/188 [==============================] - 14s 74ms/step - loss: 0.1224 - accuracy: 0.9925 - val_loss: 0.1208 - val_accuracy: 0.9939 - lr: 3.0000e-05\n",
      "Epoch 37/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.1196 - accuracy: 0.9934 - val_loss: 0.1183 - val_accuracy: 0.9947 - lr: 3.0000e-05\n",
      "Epoch 38/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.1191 - accuracy: 0.9937 - val_loss: 0.1158 - val_accuracy: 0.9944 - lr: 3.0000e-05\n",
      "Epoch 39/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.1152 - accuracy: 0.9939 - val_loss: 0.1156 - val_accuracy: 0.9933 - lr: 3.0000e-05\n",
      "Epoch 40/60\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.1140 - accuracy: 0.9943 - val_loss: 0.1171 - val_accuracy: 0.9929 - lr: 3.0000e-05\n",
      "Epoch 41/60\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.1127 - accuracy: 0.9940 - val_loss: 0.1125 - val_accuracy: 0.9948 - lr: 3.0000e-05\n",
      "Epoch 42/60\n",
      "188/188 [==============================] - 14s 74ms/step - loss: 0.1109 - accuracy: 0.9941 - val_loss: 0.1079 - val_accuracy: 0.9952 - lr: 3.0000e-05\n",
      "Epoch 43/60\n",
      "188/188 [==============================] - 14s 74ms/step - loss: 0.1112 - accuracy: 0.9935 - val_loss: 0.1064 - val_accuracy: 0.9951 - lr: 3.0000e-05\n",
      "Epoch 44/60\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.1087 - accuracy: 0.9937 - val_loss: 0.1094 - val_accuracy: 0.9940 - lr: 3.0000e-05\n",
      "Epoch 45/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.1057 - accuracy: 0.9944 - val_loss: 0.1055 - val_accuracy: 0.9947 - lr: 3.0000e-05\n",
      "Epoch 46/60\n",
      "188/188 [==============================] - 14s 74ms/step - loss: 0.1054 - accuracy: 0.9939 - val_loss: 0.1049 - val_accuracy: 0.9943 - lr: 3.0000e-05\n",
      "Epoch 47/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.1045 - accuracy: 0.9941 - val_loss: 0.1033 - val_accuracy: 0.9946 - lr: 3.0000e-06\n",
      "Epoch 48/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.1047 - accuracy: 0.9938 - val_loss: 0.1062 - val_accuracy: 0.9939 - lr: 3.0000e-06\n",
      "Epoch 49/60\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.1020 - accuracy: 0.9945 - val_loss: 0.1050 - val_accuracy: 0.9942 - lr: 3.0000e-06\n",
      "Epoch 50/60\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.1017 - accuracy: 0.9948 - val_loss: 0.1036 - val_accuracy: 0.9942 - lr: 3.0000e-06\n",
      "Epoch 51/60\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.1007 - accuracy: 0.9947 - val_loss: 0.1043 - val_accuracy: 0.9943 - lr: 3.0000e-06\n",
      "Epoch 52/60\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.1010 - accuracy: 0.9945 - val_loss: 0.1016 - val_accuracy: 0.9951 - lr: 3.0000e-06\n",
      "Epoch 53/60\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.1011 - accuracy: 0.9946 - val_loss: 0.1017 - val_accuracy: 0.9943 - lr: 3.0000e-06\n",
      "Epoch 54/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.1010 - accuracy: 0.9944 - val_loss: 0.0995 - val_accuracy: 0.9942 - lr: 3.0000e-06\n",
      "Epoch 55/60\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.1006 - accuracy: 0.9938 - val_loss: 0.1018 - val_accuracy: 0.9941 - lr: 3.0000e-06\n",
      "Epoch 56/60\n",
      "188/188 [==============================] - 13s 72ms/step - loss: 0.1005 - accuracy: 0.9940 - val_loss: 0.1007 - val_accuracy: 0.9942 - lr: 3.0000e-06\n",
      "Epoch 57/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.0987 - accuracy: 0.9948 - val_loss: 0.0988 - val_accuracy: 0.9947 - lr: 3.0000e-06\n",
      "Epoch 58/60\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.0990 - accuracy: 0.9946 - val_loss: 0.0990 - val_accuracy: 0.9943 - lr: 3.0000e-06\n",
      "Epoch 59/60\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.0988 - accuracy: 0.9944 - val_loss: 0.1001 - val_accuracy: 0.9948 - lr: 3.0000e-06\n",
      "Epoch 60/60\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.0963 - accuracy: 0.9955 - val_loss: 0.0974 - val_accuracy: 0.9952 - lr: 3.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# Train the model with augmented data and callbacks for learning rate schedule, model checkpointing, and early stopping\n",
    "history = net.fit(\n",
    "    training_data=datagen.flow(x_train, y_train, batch_size=256, subset='training'), # Training data generator \n",
    "    validation_data=datagen.flow(x_train, y_train, subset='validation'),             # Validation generator \n",
    "    epochs=60,\n",
    "    callbacks=[\n",
    "        LearningRateScheduler(lr_schedule),                    # Dynamic learning rate schedule       \n",
    "        ModelCheckpoint('model.h5', save_best_only=True),      # Save best weights to file\n",
    "        EarlyStopping(patience=12, restore_best_weights=True)  # Stop if no improvement after 12 epochs \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step - loss: 0.0900 - accuracy: 0.9964\n",
      "Test accuracy: 99.64%\n"
     ]
    }
   ],
   "source": [
    "# Load best performing version of the model, and check accuracy against test data (not used during training)\n",
    "final_model = load_model('model.h5')\n",
    "loss, acc = final_model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in HDF5 format\n",
    "final_model.save(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
